{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9eka9mrMt_P"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4oRuP9M1s27l"
      },
      "source": [
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRrGSzQCRo-A"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "import transformers\r\n",
        "from transformers import AutoModel, BertTokenizerFast\r\n",
        "\r\n",
        "# specify GPU\r\n",
        "device = torch.device(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yN_0aPAqsjx3",
        "outputId": "240c0888-d9d5-4937-db6e-1cc6702134be"
      },
      "source": [
        "from spacy.lang.ru import Russian\r\n",
        "import nltk\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from string import punctuation\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "russian_stopwords = stopwords.words(\"russian\")\r\n",
        "nlp = Russian()\r\n",
        "\r\n",
        "def preprocess_text(text):\r\n",
        "  sentence = nlp(text)\r\n",
        "  tokens = [token.text for token in sentence]\r\n",
        "  tokens = [token for token in tokens if token not in russian_stopwords\\\r\n",
        "            and token != \" \" \\\r\n",
        "            and token.strip() not in punctuation]\r\n",
        "  \r\n",
        "  text = \" \".join(tokens)\r\n",
        "  \r\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISmlPAxMNHY"
      },
      "source": [
        "train = pd.read_csv('trainSet.csv')\r\n",
        "test = pd.read_csv('testSet.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7wgfyzCtABI",
        "outputId": "4b95989d-7015-4701-dbaa-48866b818853"
      },
      "source": [
        "y_train = np.array(train['Class'])\r\n",
        "y_test = np.array(test['Class'])\r\n",
        "\r\n",
        "print(np.unique(y_train))\r\n",
        "print(np.unique(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  3  8 21 22 23 31 32 41 42 43 44]\n",
            "[ 1  3  8 21 22 23 31 32 41 42 43 44 51 52 53 54 55 56 57 58 59 60 61]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfEtQoIrtCqh",
        "outputId": "4bdb999c-a1ae-4cbc-9d3b-7f610a07f748"
      },
      "source": [
        "test = test[test['Class'] <=44]\r\n",
        "\r\n",
        "y_test = np.array(test['Class'])\r\n",
        "\r\n",
        "print(np.unique(y_train))\r\n",
        "print(np.unique(y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 1  3  8 21 22 23 31 32 41 42 43 44]\n",
            "[ 1  3  8 21 22 23 31 32 41 42 43 44]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wltsoautD54",
        "outputId": "917ff780-d228-4063-f29b-9c2f0e015dbf"
      },
      "source": [
        "d = {}\r\n",
        "inv_d = {}\r\n",
        "l = np.unique(y_train)\r\n",
        "for i in range(len(l)):\r\n",
        "  d[i] = l[i]\r\n",
        "  inv_d[l[i]] = i\r\n",
        "\r\n",
        "print(d)\r\n",
        "print(inv_d)\r\n",
        "\r\n",
        "train_labels = np.array([inv_d[i] for i in y_train])\r\n",
        "test_labels = np.array([inv_d[i] for i in y_test])\r\n",
        "print(np.unique(train_labels))\r\n",
        "print(np.unique(test_labels))\r\n",
        "print(len(train_labels))\r\n",
        "print(len(test_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 1, 1: 3, 2: 8, 3: 21, 4: 22, 5: 23, 6: 31, 7: 32, 8: 41, 9: 42, 10: 43, 11: 44}\n",
            "{1: 0, 3: 1, 8: 2, 21: 3, 22: 4, 23: 5, 31: 6, 32: 7, 41: 8, 42: 9, 43: 10, 44: 11}\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11]\n",
            "2244\n",
            "963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hzkvgPwtGJy",
        "outputId": "18cd7a5e-3d73-4da8-c0f2-175611d98e9e"
      },
      "source": [
        "t_train = train['Text'].apply(preprocess_text)\r\n",
        "test_text = test['Text'].apply(preprocess_text)\r\n",
        "\r\n",
        "print(len(t_train))\r\n",
        "print(len(test_text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2244\n",
            "963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD-h2Y9PXU4_"
      },
      "source": [
        "train_text, val_text, train_labels, val_labels = train_test_split(t_train, train_labels, \r\n",
        "                                                                    random_state=2018, \r\n",
        "                                                                    test_size=0.2, \r\n",
        "                                                                    stratify=train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC9kWGdIVGJJ"
      },
      "source": [
        "bert = AutoModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\r\n",
        "# Load the BERT tokenizer\r\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"DeepPavlov/rubert-base-cased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y85AnMPsgbIY"
      },
      "source": [
        "tokenizer.get_vocab()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "TTa1fk8E6-qh",
        "outputId": "8ce65c4d-7fe8-4135-ef36-92749f0430dc"
      },
      "source": [
        "seq_len = [len(i.split()) for i in train_text]\r\n",
        "\r\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdcd3e67f50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "1d8Xz0rx6_MQ",
        "outputId": "c9e02c95-9d53-49ec-8b20-3c8c8226e305"
      },
      "source": [
        "seq_len = [len(i.split()) for i in test_text]\r\n",
        "\r\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fdcbf93f210>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIBtJGooXgFI"
      },
      "source": [
        "max_seq_len = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eJPF3HIXkq3",
        "outputId": "b8241693-3fd7-4f6e-d7a8-7b51b82278c0"
      },
      "source": [
        "# tokenize and encode sequences in the training set\r\n",
        "tokens_train = tokenizer.batch_encode_plus(\r\n",
        "    train_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the validation set\r\n",
        "tokens_val = tokenizer.batch_encode_plus(\r\n",
        "    val_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")\r\n",
        "\r\n",
        "# tokenize and encode sequences in the test set\r\n",
        "tokens_test = tokenizer.batch_encode_plus(\r\n",
        "    test_text.tolist(),\r\n",
        "    max_length = max_seq_len,\r\n",
        "    pad_to_max_length=True,\r\n",
        "    truncation=True,\r\n",
        "    return_token_type_ids=False\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2155: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSUiOewBXtOo"
      },
      "source": [
        "# for train set\r\n",
        "train_seq = torch.tensor(tokens_train['input_ids'])\r\n",
        "train_mask = torch.tensor(tokens_train['attention_mask'])\r\n",
        "train_y = torch.tensor(train_labels.tolist())\r\n",
        "\r\n",
        "# for validation set\r\n",
        "val_seq = torch.tensor(tokens_val['input_ids'])\r\n",
        "val_mask = torch.tensor(tokens_val['attention_mask'])\r\n",
        "val_y = torch.tensor(val_labels.tolist())\r\n",
        "\r\n",
        "# for test set\r\n",
        "test_seq = torch.tensor(tokens_test['input_ids'])\r\n",
        "test_mask = torch.tensor(tokens_test['attention_mask'])\r\n",
        "test_y = torch.tensor(test_labels.tolist())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZwKZcePXxS_"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "#define a batch size\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "\r\n",
        "# dataLoader for train set\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "val_sampler = SequentialSampler(val_data)\r\n",
        "\r\n",
        "# dataLoader for validation set\r\n",
        "val_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VyEAdN2X0jR"
      },
      "source": [
        "# freeze all the parameters\r\n",
        "for param in bert.parameters():\r\n",
        "    param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ASNjV-QX4Zg"
      },
      "source": [
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "  def __init__(self, bert):\r\n",
        "    \r\n",
        "    super(BERT_Arch, self).__init__()\r\n",
        "\r\n",
        "    self.bert = bert \r\n",
        "    \r\n",
        "    # dropout layer\r\n",
        "    self.dropout = nn.Dropout(0.1)\r\n",
        "    \r\n",
        "    # relu activation function\r\n",
        "    self.relu =  nn.ReLU()\r\n",
        "\r\n",
        "    # dense layer 1\r\n",
        "    self.fc1 = nn.Linear(768,512)\r\n",
        "    \r\n",
        "    # dense layer 2 (Output layer)\r\n",
        "    self.fc2 = nn.Linear(512,12)\r\n",
        "\r\n",
        "    #softmax activation function\r\n",
        "    self.softmax = nn.Softmax(dim=1)\r\n",
        "\r\n",
        "  #define the forward pass\r\n",
        "  def forward(self, sent_id, mask):\r\n",
        "\r\n",
        "    #pass the inputs to the model  \r\n",
        "    cls_hs = self.bert(sent_id, attention_mask=mask)[1]\r\n",
        "\r\n",
        "    x = self.fc1(cls_hs)\r\n",
        "\r\n",
        "    x = self.relu(x)\r\n",
        "\r\n",
        "    x = self.dropout(x)\r\n",
        "\r\n",
        "    # output layer\r\n",
        "    x = self.fc2(x)\r\n",
        "    \r\n",
        "    # apply softmax activation\r\n",
        "    #x = self.softmax(x)\r\n",
        "\r\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFh5aya0X4YY"
      },
      "source": [
        "# pass the pre-trained BERT to our define architecture\r\n",
        "model = BERT_Arch(bert)\r\n",
        "\r\n",
        "# push the model to GPU\r\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gImwyLBBYOSI"
      },
      "source": [
        "from transformers import AdamW\r\n",
        "\r\n",
        "# define the optimizer\r\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8YO24KDZA4J",
        "outputId": "1ca79c04-6206-4e9c-a24f-df54a74508e1"
      },
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "#compute the class weights\r\n",
        "class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
        "\r\n",
        "print(class_wts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.69573643 0.92335391 0.7872807  0.7872807  1.03877315 1.2570028\n",
            " 1.02454338 1.0608747  0.63652482 1.6259058  1.35984848 2.93300654]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uaqkee5IZMk4"
      },
      "source": [
        "# convert class weights to tensor\r\n",
        "weights= torch.tensor(class_wts,dtype=torch.float)\r\n",
        "weights = weights.to(device)\r\n",
        "\r\n",
        "# loss function\r\n",
        "#cross_entropy  = nn.NLLLoss(weight=weights) \r\n",
        "cross_entropy = nn.CrossEntropyLoss(weight=weights)\r\n",
        "# number of training epochs\r\n",
        "epochs = 150"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUvOvg-ZZZgQ"
      },
      "source": [
        "# function to train the model\r\n",
        "def train():\r\n",
        "  \r\n",
        "  model.train()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save model predictions\r\n",
        "  total_preds=[]\r\n",
        "  \r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(train_dataloader):\r\n",
        "    \r\n",
        "    # progress update after every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [r.to(device) for r in batch]\r\n",
        " \r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # clear previously calculated gradients \r\n",
        "    model.zero_grad()        \r\n",
        "\r\n",
        "    # get model predictions for the current batch\r\n",
        "    preds = model(sent_id, mask)\r\n",
        "\r\n",
        "    # compute the loss between actual and predicted values\r\n",
        "    loss = cross_entropy(preds, labels)\r\n",
        "\r\n",
        "    # add on to the total loss\r\n",
        "    total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "    # backward pass to calculate the gradients\r\n",
        "    loss.backward()\r\n",
        "\r\n",
        "    # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "    # update parameters\r\n",
        "    optimizer.step()\r\n",
        "\r\n",
        "    # model predictions are stored on GPU. So, push it to CPU\r\n",
        "    preds=preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "    # append the model predictions\r\n",
        "    total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the training loss of the epoch\r\n",
        "  avg_loss = total_loss / len(train_dataloader)\r\n",
        "  \r\n",
        "  # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  #returns the loss and predictions\r\n",
        "  return avg_loss, total_preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgcuWxsmZctf"
      },
      "source": [
        "def evaluate():\r\n",
        "  \r\n",
        "  print(\"\\nEvaluating...\")\r\n",
        "  \r\n",
        "  # deactivate dropout layers\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  total_loss, total_accuracy = 0, 0\r\n",
        "  \r\n",
        "  # empty list to save the model predictions\r\n",
        "  total_preds = []\r\n",
        "\r\n",
        "  # iterate over batches\r\n",
        "  for step,batch in enumerate(val_dataloader):\r\n",
        "    \r\n",
        "    # Progress update every 50 batches.\r\n",
        "    if step % 50 == 0 and not step == 0:\r\n",
        "      \r\n",
        "      # Calculate elapsed time in minutes.\r\n",
        "      elapsed = format_time(time.time() - t0)\r\n",
        "            \r\n",
        "      # Report progress.\r\n",
        "      print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))\r\n",
        "\r\n",
        "    # push the batch to gpu\r\n",
        "    batch = [t.to(device) for t in batch]\r\n",
        "\r\n",
        "    sent_id, mask, labels = batch\r\n",
        "\r\n",
        "    # deactivate autograd\r\n",
        "    with torch.no_grad():\r\n",
        "      \r\n",
        "      # model predictions\r\n",
        "      preds = model(sent_id, mask)\r\n",
        "\r\n",
        "      # compute the validation loss between actual and predicted values\r\n",
        "      loss = cross_entropy(preds,labels)\r\n",
        "\r\n",
        "      total_loss = total_loss + loss.item()\r\n",
        "\r\n",
        "      preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "      total_preds.append(preds)\r\n",
        "\r\n",
        "  # compute the validation loss of the epoch\r\n",
        "  avg_loss = total_loss / len(val_dataloader) \r\n",
        "\r\n",
        "  # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "  total_preds  = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "  return avg_loss, total_preds\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ta1QeXBZi34",
        "outputId": "dfee510b-57c6-41af-8d93-01b919d07d3e"
      },
      "source": [
        "# set initial loss to infinite\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "# empty lists to store training and validation loss of each epoch\r\n",
        "train_losses=[]\r\n",
        "valid_losses=[]\r\n",
        "\r\n",
        "#for each epoch\r\n",
        "for epoch in range(epochs):\r\n",
        "     \r\n",
        "  print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\r\n",
        "  \r\n",
        "  #train model\r\n",
        "  train_loss, _ = train()\r\n",
        "  \r\n",
        "  #evaluate model\r\n",
        "  valid_loss, _ = evaluate()\r\n",
        "  \r\n",
        "  #save the best model\r\n",
        "  if valid_loss < best_valid_loss:\r\n",
        "      best_valid_loss = valid_loss\r\n",
        "      torch.save(model.state_dict(), 'saved_weights.pt')\r\n",
        "  \r\n",
        "  # append training and validation loss\r\n",
        "  train_losses.append(train_loss)\r\n",
        "  valid_losses.append(valid_loss)\r\n",
        "  \r\n",
        "  print(f'\\nTraining Loss: {train_loss:.3f}')\r\n",
        "  print(f'Validation Loss: {valid_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Epoch 1 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.393\n",
            "Validation Loss: 2.240\n",
            "\n",
            " Epoch 2 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 2.167\n",
            "Validation Loss: 2.014\n",
            "\n",
            " Epoch 3 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.925\n",
            "Validation Loss: 1.738\n",
            "\n",
            " Epoch 4 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.704\n",
            "Validation Loss: 1.530\n",
            "\n",
            " Epoch 5 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.557\n",
            "Validation Loss: 1.415\n",
            "\n",
            " Epoch 6 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.442\n",
            "Validation Loss: 1.371\n",
            "\n",
            " Epoch 7 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.393\n",
            "Validation Loss: 1.259\n",
            "\n",
            " Epoch 8 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.348\n",
            "Validation Loss: 1.112\n",
            "\n",
            " Epoch 9 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.258\n",
            "Validation Loss: 1.041\n",
            "\n",
            " Epoch 10 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.186\n",
            "Validation Loss: 1.006\n",
            "\n",
            " Epoch 11 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.212\n",
            "Validation Loss: 0.999\n",
            "\n",
            " Epoch 12 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.136\n",
            "Validation Loss: 1.012\n",
            "\n",
            " Epoch 13 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.116\n",
            "Validation Loss: 0.949\n",
            "\n",
            " Epoch 14 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.051\n",
            "Validation Loss: 0.859\n",
            "\n",
            " Epoch 15 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.067\n",
            "Validation Loss: 0.938\n",
            "\n",
            " Epoch 16 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.042\n",
            "Validation Loss: 0.987\n",
            "\n",
            " Epoch 17 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.014\n",
            "Validation Loss: 0.865\n",
            "\n",
            " Epoch 18 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.980\n",
            "Validation Loss: 0.843\n",
            "\n",
            " Epoch 19 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.017\n",
            "Validation Loss: 0.851\n",
            "\n",
            " Epoch 20 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 1.001\n",
            "Validation Loss: 0.835\n",
            "\n",
            " Epoch 21 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.949\n",
            "Validation Loss: 0.814\n",
            "\n",
            " Epoch 22 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.942\n",
            "Validation Loss: 0.776\n",
            "\n",
            " Epoch 23 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.962\n",
            "Validation Loss: 0.768\n",
            "\n",
            " Epoch 24 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.954\n",
            "Validation Loss: 0.750\n",
            "\n",
            " Epoch 25 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.938\n",
            "Validation Loss: 0.767\n",
            "\n",
            " Epoch 26 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.909\n",
            "Validation Loss: 0.754\n",
            "\n",
            " Epoch 27 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.855\n",
            "Validation Loss: 0.843\n",
            "\n",
            " Epoch 28 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.885\n",
            "Validation Loss: 0.741\n",
            "\n",
            " Epoch 29 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.911\n",
            "Validation Loss: 0.705\n",
            "\n",
            " Epoch 30 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.890\n",
            "Validation Loss: 0.709\n",
            "\n",
            " Epoch 31 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.843\n",
            "Validation Loss: 0.716\n",
            "\n",
            " Epoch 32 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.848\n",
            "Validation Loss: 0.741\n",
            "\n",
            " Epoch 33 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.877\n",
            "Validation Loss: 0.767\n",
            "\n",
            " Epoch 34 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.807\n",
            "Validation Loss: 0.665\n",
            "\n",
            " Epoch 35 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.815\n",
            "Validation Loss: 0.654\n",
            "\n",
            " Epoch 36 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.836\n",
            "Validation Loss: 0.737\n",
            "\n",
            " Epoch 37 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.831\n",
            "Validation Loss: 0.724\n",
            "\n",
            " Epoch 38 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.798\n",
            "Validation Loss: 0.657\n",
            "\n",
            " Epoch 39 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.827\n",
            "Validation Loss: 0.676\n",
            "\n",
            " Epoch 40 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.774\n",
            "Validation Loss: 0.695\n",
            "\n",
            " Epoch 41 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.823\n",
            "Validation Loss: 0.642\n",
            "\n",
            " Epoch 42 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.759\n",
            "Validation Loss: 0.675\n",
            "\n",
            " Epoch 43 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.788\n",
            "Validation Loss: 0.657\n",
            "\n",
            " Epoch 44 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.793\n",
            "Validation Loss: 0.736\n",
            "\n",
            " Epoch 45 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.787\n",
            "Validation Loss: 0.679\n",
            "\n",
            " Epoch 46 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.780\n",
            "Validation Loss: 0.635\n",
            "\n",
            " Epoch 47 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.796\n",
            "Validation Loss: 0.630\n",
            "\n",
            " Epoch 48 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.792\n",
            "Validation Loss: 0.644\n",
            "\n",
            " Epoch 49 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.755\n",
            "Validation Loss: 0.672\n",
            "\n",
            " Epoch 50 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.747\n",
            "Validation Loss: 0.670\n",
            "\n",
            " Epoch 51 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.721\n",
            "Validation Loss: 0.686\n",
            "\n",
            " Epoch 52 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.747\n",
            "Validation Loss: 0.632\n",
            "\n",
            " Epoch 53 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.724\n",
            "Validation Loss: 0.661\n",
            "\n",
            " Epoch 54 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.765\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 55 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.699\n",
            "Validation Loss: 0.637\n",
            "\n",
            " Epoch 56 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.717\n",
            "Validation Loss: 0.640\n",
            "\n",
            " Epoch 57 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.718\n",
            "Validation Loss: 0.670\n",
            "\n",
            " Epoch 58 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.722\n",
            "Validation Loss: 0.643\n",
            "\n",
            " Epoch 59 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.711\n",
            "Validation Loss: 0.630\n",
            "\n",
            " Epoch 60 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.724\n",
            "Validation Loss: 0.620\n",
            "\n",
            " Epoch 61 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.706\n",
            "Validation Loss: 0.577\n",
            "\n",
            " Epoch 62 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.691\n",
            "Validation Loss: 0.609\n",
            "\n",
            " Epoch 63 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.664\n",
            "Validation Loss: 0.583\n",
            "\n",
            " Epoch 64 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.675\n",
            "Validation Loss: 0.621\n",
            "\n",
            " Epoch 65 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.689\n",
            "Validation Loss: 0.711\n",
            "\n",
            " Epoch 66 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.701\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 67 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.679\n",
            "Validation Loss: 0.614\n",
            "\n",
            " Epoch 68 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.714\n",
            "Validation Loss: 0.639\n",
            "\n",
            " Epoch 69 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.671\n",
            "Validation Loss: 0.641\n",
            "\n",
            " Epoch 70 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.687\n",
            "Validation Loss: 0.613\n",
            "\n",
            " Epoch 71 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.706\n",
            "Validation Loss: 0.588\n",
            "\n",
            " Epoch 72 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.685\n",
            "Validation Loss: 0.580\n",
            "\n",
            " Epoch 73 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.649\n",
            "Validation Loss: 0.619\n",
            "\n",
            " Epoch 74 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.649\n",
            "Validation Loss: 0.587\n",
            "\n",
            " Epoch 75 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.658\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 76 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.686\n",
            "Validation Loss: 0.580\n",
            "\n",
            " Epoch 77 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.684\n",
            "Validation Loss: 0.596\n",
            "\n",
            " Epoch 78 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.657\n",
            "Validation Loss: 0.647\n",
            "\n",
            " Epoch 79 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.647\n",
            "Validation Loss: 0.558\n",
            "\n",
            " Epoch 80 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.645\n",
            "Validation Loss: 0.607\n",
            "\n",
            " Epoch 81 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.626\n",
            "Validation Loss: 0.629\n",
            "\n",
            " Epoch 82 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.680\n",
            "Validation Loss: 0.559\n",
            "\n",
            " Epoch 83 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.654\n",
            "Validation Loss: 0.526\n",
            "\n",
            " Epoch 84 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.679\n",
            "Validation Loss: 0.612\n",
            "\n",
            " Epoch 85 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.671\n",
            "Validation Loss: 0.634\n",
            "\n",
            " Epoch 86 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.672\n",
            "Validation Loss: 0.548\n",
            "\n",
            " Epoch 87 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.653\n",
            "Validation Loss: 0.665\n",
            "\n",
            " Epoch 88 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.612\n",
            "Validation Loss: 0.637\n",
            "\n",
            " Epoch 89 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.624\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 90 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.632\n",
            "Validation Loss: 0.607\n",
            "\n",
            " Epoch 91 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.625\n",
            "Validation Loss: 0.605\n",
            "\n",
            " Epoch 92 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.601\n",
            "Validation Loss: 0.681\n",
            "\n",
            " Epoch 93 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.672\n",
            "Validation Loss: 0.603\n",
            "\n",
            " Epoch 94 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.628\n",
            "Validation Loss: 0.557\n",
            "\n",
            " Epoch 95 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.617\n",
            "Validation Loss: 0.560\n",
            "\n",
            " Epoch 96 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.628\n",
            "Validation Loss: 0.575\n",
            "\n",
            " Epoch 97 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.627\n",
            "Validation Loss: 0.567\n",
            "\n",
            " Epoch 98 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.616\n",
            "Validation Loss: 0.534\n",
            "\n",
            " Epoch 99 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.575\n",
            "Validation Loss: 0.551\n",
            "\n",
            " Epoch 100 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.597\n",
            "Validation Loss: 0.547\n",
            "\n",
            " Epoch 101 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.590\n",
            "Validation Loss: 0.603\n",
            "\n",
            " Epoch 102 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.609\n",
            "Validation Loss: 0.564\n",
            "\n",
            " Epoch 103 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.622\n",
            "Validation Loss: 0.627\n",
            "\n",
            " Epoch 104 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.613\n",
            "Validation Loss: 0.538\n",
            "\n",
            " Epoch 105 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.591\n",
            "Validation Loss: 0.531\n",
            "\n",
            " Epoch 106 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.648\n",
            "Validation Loss: 0.570\n",
            "\n",
            " Epoch 107 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.620\n",
            "Validation Loss: 0.615\n",
            "\n",
            " Epoch 108 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.613\n",
            "Validation Loss: 0.574\n",
            "\n",
            " Epoch 109 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.599\n",
            "Validation Loss: 0.648\n",
            "\n",
            " Epoch 110 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.611\n",
            "Validation Loss: 0.600\n",
            "\n",
            " Epoch 111 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.614\n",
            "Validation Loss: 0.667\n",
            "\n",
            " Epoch 112 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.593\n",
            "Validation Loss: 0.596\n",
            "\n",
            " Epoch 113 / 150\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.564\n",
            "Validation Loss: 0.660\n",
            "\n",
            " Epoch 114 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.616\n",
            "Validation Loss: 0.578\n",
            "\n",
            " Epoch 115 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.569\n",
            "Validation Loss: 0.575\n",
            "\n",
            " Epoch 116 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.622\n",
            "Validation Loss: 0.588\n",
            "\n",
            " Epoch 117 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.598\n",
            "Validation Loss: 0.571\n",
            "\n",
            " Epoch 118 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.602\n",
            "Validation Loss: 0.569\n",
            "\n",
            " Epoch 119 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.572\n",
            "Validation Loss: 0.614\n",
            "\n",
            " Epoch 120 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.602\n",
            "Validation Loss: 0.610\n",
            "\n",
            " Epoch 121 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.578\n",
            "Validation Loss: 0.571\n",
            "\n",
            " Epoch 122 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.561\n",
            "Validation Loss: 0.579\n",
            "\n",
            " Epoch 123 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.625\n",
            "Validation Loss: 0.560\n",
            "\n",
            " Epoch 124 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.569\n",
            "Validation Loss: 0.675\n",
            "\n",
            " Epoch 125 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.546\n",
            "Validation Loss: 0.611\n",
            "\n",
            " Epoch 126 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.591\n",
            "Validation Loss: 0.649\n",
            "\n",
            " Epoch 127 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.578\n",
            "Validation Loss: 0.564\n",
            "\n",
            " Epoch 128 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.592\n",
            "Validation Loss: 0.595\n",
            "\n",
            " Epoch 129 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.587\n",
            "Validation Loss: 0.554\n",
            "\n",
            " Epoch 130 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.559\n",
            "Validation Loss: 0.543\n",
            "\n",
            " Epoch 131 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.533\n",
            "Validation Loss: 0.533\n",
            "\n",
            " Epoch 132 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.530\n",
            "Validation Loss: 0.593\n",
            "\n",
            " Epoch 133 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.551\n",
            "Validation Loss: 0.593\n",
            "\n",
            " Epoch 134 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.542\n",
            "Validation Loss: 0.534\n",
            "\n",
            " Epoch 135 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.564\n",
            "Validation Loss: 0.606\n",
            "\n",
            " Epoch 136 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.563\n",
            "Validation Loss: 0.644\n",
            "\n",
            " Epoch 137 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.587\n",
            "Validation Loss: 0.625\n",
            "\n",
            " Epoch 138 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.570\n",
            "Validation Loss: 0.603\n",
            "\n",
            " Epoch 139 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.536\n",
            "Validation Loss: 0.621\n",
            "\n",
            " Epoch 140 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.610\n",
            "Validation Loss: 0.574\n",
            "\n",
            " Epoch 141 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.560\n",
            "Validation Loss: 0.577\n",
            "\n",
            " Epoch 142 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.569\n",
            "Validation Loss: 0.581\n",
            "\n",
            " Epoch 143 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.545\n",
            "Validation Loss: 0.558\n",
            "\n",
            " Epoch 144 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.512\n",
            "Validation Loss: 0.633\n",
            "\n",
            " Epoch 145 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.550\n",
            "Validation Loss: 0.586\n",
            "\n",
            " Epoch 146 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.574\n",
            "Validation Loss: 0.647\n",
            "\n",
            " Epoch 147 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.530\n",
            "Validation Loss: 0.593\n",
            "\n",
            " Epoch 148 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.575\n",
            "Validation Loss: 0.553\n",
            "\n",
            " Epoch 149 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.534\n",
            "Validation Loss: 0.531\n",
            "\n",
            " Epoch 150 / 150\n",
            "  Batch    50  of     57.\n",
            "\n",
            "Evaluating...\n",
            "\n",
            "Training Loss: 0.568\n",
            "Validation Loss: 0.597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKJYSY2nfPla"
      },
      "source": [
        "# get predictions for test data\r\n",
        "with torch.no_grad():\r\n",
        "  preds = model(test_seq.to(device), test_mask.to(device))\r\n",
        "  preds = preds.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLn_AdJVfrR4",
        "outputId": "41f95099-b366-4b33-d53e-ff54d69f5104"
      },
      "source": [
        "# model's performance\r\n",
        "preds = np.argmax(preds, axis = 1)\r\n",
        "print(classification_report(test_y, preds))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93       122\n",
            "           1       0.84      0.90      0.87        90\n",
            "           2       0.92      0.67      0.77        81\n",
            "           3       0.86      0.76      0.81       116\n",
            "           4       0.76      0.90      0.83        93\n",
            "           5       0.73      0.83      0.78        59\n",
            "           6       0.75      0.86      0.80        76\n",
            "           7       0.88      0.77      0.82        83\n",
            "           8       0.77      0.84      0.81       109\n",
            "           9       0.83      0.67      0.74        45\n",
            "          10       0.87      0.69      0.77        58\n",
            "          11       0.56      0.71      0.63        31\n",
            "\n",
            "    accuracy                           0.82       963\n",
            "   macro avg       0.81      0.80      0.80       963\n",
            "weighted avg       0.82      0.82      0.82       963\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V8A4fsE5n0xI"
      },
      "source": [
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDLxfZ4nchkp",
        "outputId": "2624a900-94cf-4f74-f721-0baefcde7c00"
      },
      "source": [
        "f1_score(test_y, preds, average = 'macro')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7959158315916398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aGbGI9OcnZ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}